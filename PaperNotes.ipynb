{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trust Me, I'm Partially Right: Incremental Visualization Let's Analysts Explore Large Data Faster\n",
    "\n",
    "## Abstract\n",
    "\n",
    "Introduce user interface simulator sampleAction to explore whether users can be nudged to trush sampled views/visualizations of the data. Trying to find UI design elements which promote allow to user to both trust what they are observing and explore data interactively.\n",
    "\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Users have resorted to batch era processing in the era of Big Data. How can we give back interactive interfaces for data exploration?\n",
    "\n",
    "'There is an important interaction issue here'. Anaylysts are accustomed to seeing precise figures, rather thatn probalistic results, and may not be willing to act on partial information. CI add a degree of complexity to a visualization (and maybe simply confusing). In order for incremental analysis to be a viable technique, it will be important to understand how analyst actaully interact with incremental data.'\n",
    "\n",
    "They wish ...\n",
    "\n",
    "1) test an front-end application which allows the user to experience iterative, converging estimates in their own data\n",
    "2) understand how the interaction with #1 enables exploratory analysis\n",
    "\n",
    "\n",
    "Introduce sampleAction to explort #1 and #2. This is a simulator, so no back-end technicals. Test with 3 different teams. Found that sampleAction prompted them to discard queries and try new ones on the fly.\n",
    "\n",
    "Main Contributions...\n",
    "\n",
    "- methodology for simulating aggregate queries against large data back-ends\n",
    "- observations of expert analyst behaviour in interacting with approximate queries \n",
    "\n",
    "## Background....\n",
    "\n",
    "'A dataset can be thought of as a table of data, made up of measures - the values to be visualized - and dimensions, the categories into which the measures are divided.'\n",
    "\n",
    "There is a precision vs speed tradeoff which correlates to batch vs interaction.\n",
    "\n",
    "'Tableau has an ability to handle samples from a large dataset selected at random. Tableau then allows rapid queries against the in-memory portion of the dataset. These queries can be interactive, but, as they are based on a sample, they cannot be precise. Tableua does not provide a way for the user to know how representative the sample is of the full dataset/population.'\n",
    "\n",
    "\n",
    "Incremental analysis collects ever larger samples in the back-end, and uses them to estimate the true value of a query. Approximate queries can also provide CI.\n",
    "\n",
    "This project uses uncertainty visualizations over time to monitor estimates on incremental data. First user study testing it.\n",
    "\n",
    "\n",
    "#### Your Thoughts\n",
    "This is the crux of your project at the moment. Converging to some objective function incrementally over time with more and more samples.\n",
    "\n",
    "\n",
    "## Method\n",
    "\n",
    "Hypothesis: Users working with incremental visualizations will be able to inerpret the CI intervals comfortably. This will further encourage them to act rapidly on their queries. Last, hypothesize that incremental results will allow users to carry out exploratory queries.\n",
    "\n",
    "### Experimentation\n",
    "\n",
    "To test the above, don't need full scale database. (This is good for our project btws :) ). Rather need to produce realistic experience that allows users to understand what using an incremental database is like.\n",
    "\n",
    "sampleAction is simulator which acts on users own data. It allows users to formulate queries visually. The system responds with partial result, dispalying a bar chart with confidence bounds. As the anaylyst waits, the system increases its sample size, narrowing the CI intervals yielding a more precise result.\n",
    "\n",
    "### System Implementation (Random Notes)\n",
    "\n",
    "The choice of appropriate bounds is at the heart of the sampleAction system. Bounds should approriately frame the data. They need to represent the highest and lowest likely values of the ground truth. If the bounds are too wide, the user will gain little information about their estimate. If the bounds converge too slowly, incremental visualization will be little better than waiting overnight for the result. Computing statistically accurate bounds requires random samples from the dataset in order to ensure that the sample is unbiased. As a result, incremental results need to be selected from a randomly-ordered stream. This is a hard, well studied problem in databases. Assume data is randomly sampled.\n",
    "\n",
    "Given a stream of samples, sampleAction computer an estimate of the expected value of an aggregate on the stream. The uses the rows processed so far in order to make an estimate of the value based on the full dataset. \n",
    "\n",
    "With random streams, analyst will have to wait for confident answers on rare categories.\n",
    "\n",
    "For sampleAction to work against very large dataset, we want the formulat that provides CI bounds to be scalable. In particular, we would not want that size to generate large confidence intervals. We also want an estimator in which the confidence bounds narrow monotonically as the sample size decreases.\n",
    "\n",
    "The computation of appropriate bounds is an acive area of research in probability theory and different bounds are approriate under difference circumstances. Some estimators gain their strenght by using addiional information from the dataset beyond sampled values. For instance, its common to examine the minumn and maximum values in a data column. The pace at which the bounds shrink is determined by the size and the variance of the sample. The bounds expand with the variance of the sample, and tighten in proportion of the square root of the number of samples. As a result, the choice of estimator combined with the distribution in the results can produce very different bounds, changing at very different speeds.\n",
    "\n",
    "In the sampleAction prototype, we computed serveral different sets of bound in order to learn about their convergence properties.\n",
    "\n",
    "### Users Studies\n",
    "\n",
    "- Server Farm\n",
    "- Gaming\n",
    "- Twitter\n",
    "\n",
    "There is a narative structure describing all 3. Very entertaining and informative.\n",
    "\n",
    "### Analysis\n",
    "\n",
    "#### The Value of Seeing the First Record Fast\n",
    "\n",
    "- Users found errors in their initial query quickly rather than waiting hours for batch job to fail.\n",
    "- Uniform distributions allowed quick confidence.\n",
    "- Can see how dimensions are lining up.\n",
    "\n",
    "#### New Behaviour Around Data\n",
    "\n",
    "- Users immediately changed their behavior. Asking new queries they never imagined.\n",
    "- Encouraged real exploration of the data.\n",
    "\n",
    "#### Difficulties wiht Error Bound Convergence\n",
    "\n",
    "- Authors did not anticipate the tremendour variance in confidence interval sizes.\n",
    "- Bob never saw a CI larger than his largest value. Allan often could not see his data without hiding the CI. Past literature on visuaulizing uncertainty has emphasized visualization that fit the entire uncertainty region on screen. These were not sufficient for some of these preliminary bounds. It would be worthwhile to investigate visualizations that can show the size of the interval event past screen borders.\n",
    "- Incremental datasets can be slowed by unclean data. Need domain knowledge to detect these.\n",
    "\n",
    "#### Non-Expert Views of CI\n",
    "\n",
    "- Users did not realize that they would shrink towards the ground truth as more samples arrived.\n",
    "- CI is a complext indicator. It carries information about boht the number of samples seen so far and the variance of a column.\n",
    "- This means that tow adjacent columns might have identical confidence intervals: one has a small variance but is rare in the datadata, the other is common but has many samples. Together, they create the same visual bounds. Distinguishing the cases would be useful. This is normal for databases b/c rare categories/dimensions are prevalent.\n",
    "\n",
    "##### Implication\n",
    "\n",
    "Users seem able to interpret CI which opens avenue to using uncertainty visualization tied to probabilistic datasets. Need more Database support this.\n",
    "\n",
    "##### Limitations\n",
    "\n",
    "- Outliers -> can't sample an outlier\n",
    "- Joins\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "Shown it is both tractable and desirable to support incremental query interaction for data analysts. Immediate feedback is invaluable. Updating error bounds (convergence) allowed anaylysts to trust their decision points saving hours of effort.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.6.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
